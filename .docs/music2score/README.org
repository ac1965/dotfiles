#+TITLE: music2score — Audio→Stems→MIDI→Lyrics→Score Pipeline
#+AUTHOR: YAMASHITA, Takao
#+OPTIONS: toc:3 num:nil
#+PROPERTY: header-args :noweb no-export :mkdirp yes

* Overview
本プロジェクトは、音源ファイルから以下を自動生成するパイプラインです。

1. Demucs によるステム分離（複数モデル＋アンサンブル対応）
2. Basic Pitch による各パート MIDI 生成
3. faster-whisper による word-level 歌詞抽出
4. music21 による MusicXML スコア生成
5. MuseScore CLI による PDF スコア出力

かつ、以下の環境ごとに =README.org= から tangle して構成できます。

- macOS
- WSL2 (Ubuntu)
- Docker（単体／docker compose）

共通コード・依存関係は =env/common/= に 出力し、
環境固有のファイルは =env/macOS/=, =env/wsl2/=, =env/docker/= に出力します。

* 想定ディレクトリ構成

#+begin_src text
  project-root/
    README.org
    input/
      foo.wav        # 入力音源
    out/             # 出力（自動生成）
    env/
      common/
        requirements.txt
        scripts/
          process.sh
          ensemble_stems.py
          align_musicxml.py
          asr.py
      macOS/
        Makefile
      wsl2/
        Makefile
      docker/
        Dockerfile
        Makefile
        docker-compose.yml
#+end_src

この README.org を保存後、=org-babel-tangle= すれば上記ファイル群が生成されます。

* 共通環境（env/common）

** requirements.txt（共通依存）

#+begin_src conf :tangle env/common/requirements.txt
  demucs>=4.0.0
  torch
  torchaudio==2.3.1

  basic-pitch==0.4.0
  numpy==1.26.4
  soundfile==0.12.1
  librosa==0.10.2

  faster-whisper==1.0.3
  music21==9.1.0

  scikit-learn<=1.5.1
  tqdm==4.66.5
#+end_src

** process.sh（共通パイプライン本体）

#+begin_src sh :tangle env/common/scripts/process.sh :shebang "#!/usr/bin/env bash"
  set -euo pipefail
  export TORCHAUDIO_USE_SOUNDFILE=1

  # === Args ===
  AUDIO_IN="${1:-}"
  : "${AUDIO_IN:?Usage: $0 <audio.(wav|aif|aiff|aifc|mp3|flac)>}"

  # === Paths ===
  SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
  WORK_DIR="$(cd "$SCRIPT_DIR/../.." && pwd)"   # project-root
  OUT_DIR="$WORK_DIR/out"
  TMP_DIR="$OUT_DIR/tmp_audio"
  STEM_ROOT="$OUT_DIR/stems"
  ENS_DIR="$OUT_DIR/stems_ens"
  MIDI_DIR="$OUT_DIR/midi"
  LYRICS_DIR="$OUT_DIR/lyrics"
  SCORE_DIR="$OUT_DIR/score"
  SCORE_XML="$SCORE_DIR/score.musicxml"
  SCORE_PDF="$SCORE_DIR/score.pdf"

  MODELS="${MODELS:-htdemucs}"  # e.g. "htdemucs htdemucs_6s"
  ENSEMBLE_METHOD="${ENSEMBLE_METHOD:-median}" # median | tmean
  TMEAN_ALPHA="${TMEAN_ALPHA:-0.1}"            # used when ENSEMBLE_METHOD=tmean

  mkdir -p "$OUT_DIR" "$TMP_DIR" "$STEM_ROOT" "$ENS_DIR" \
    "$MIDI_DIR" "$LYRICS_DIR" "$SCORE_DIR"

  # === 0) Normalize input ===
  BASE="$(basename "$AUDIO_IN")"
  BASE_NOEXT="${BASE%.*}"
  WAV_IN="$TMP_DIR/${BASE_NOEXT}.wav"

  echo "[normalize] $AUDIO_IN -> $WAV_IN"
  ffmpeg -y -i "$AUDIO_IN" -ac 2 -ar 44100 -sample_fmt s16 \
    "$WAV_IN" >/dev/null 2>&1

  # === 1) MODELS parsing ===
  MODELS_SANE="$(printf "%s" "$MODELS" | tr '、,' ' ' | tr -s '[:space:]' ' ')"
  read -r -a MODEL_ARR <<< "$MODELS_SANE"
  if [ "${#MODEL_ARR[@]}" -eq 0 ]; then
    MODEL_ARR=(htdemucs)
  fi
  echo "[models] parsed: ${#MODEL_ARR[@]} -> ${MODEL_ARR[*]}"

  # === 2) Demucs per-model ===
  for m in "${MODEL_ARR[@]}"; do
    echo "[demucs] model=$m"
    OUT_SUBDIR="$STEM_ROOT/$m"
    mkdir -p "$OUT_SUBDIR"
    echo "  -> demucs -n $m -o $OUT_SUBDIR $WAV_IN"
    demucs -n "$m" -o "$OUT_SUBDIR" "$WAV_IN"
  done

  # === 3) Ensemble (if >=2 models) ===
  if [ "${#MODEL_ARR[@]}" -ge 2 ]; then
    echo "[ensemble] models=${MODEL_ARR[*]} method=$ENSEMBLE_METHOD alpha=$TMEAN_ALPHA"
    python "$SCRIPT_DIR/ensemble_stems.py" \
      --models "$MODELS_SANE" \
      --stem-root "$STEM_ROOT" \
      --track "$BASE_NOEXT" \
      --out-dir "$ENS_DIR" \
      --method "$ENSEMBLE_METHOD" \
      --tmean-alpha "$TMEAN_ALPHA"
    STEM_DIR="$ENS_DIR/$BASE_NOEXT"
  else
    STEM_DIR="$STEM_ROOT/${MODEL_ARR[0]}/$BASE_NOEXT"
  fi

  echo "[stems] using STEM_DIR=$STEM_DIR"

  # === 4) Basic Pitch (per stem) ===
  mkdir -p "$MIDI_DIR"

  bp_conv() {
    local f="$1"
    if [ -f "$f" ]; then
      echo "[basic-pitch] $f"
      basic-pitch \
        --save-midi \
        --no-melodia \
        "$MIDI_DIR" \
        "$f" >/dev/null 2>&1 || true
    fi
  }

  for stem in vocals bass drums other guitar piano; do
    bp_conv "$STEM_DIR/$stem.wav"
  done

  # === 5) ASR (faster-whisper) ===
  LYRICS_JSON="$LYRICS_DIR/lyrics_words.json"
  ASR_SRC="$STEM_DIR/vocals.wav"
  [ ! -f "$ASR_SRC" ] && ASR_SRC="$WAV_IN"

  echo "[asr] source=$ASR_SRC"
  PYTHONIOENCODING=utf-8 \
  ASR_SRC="$ASR_SRC" \
  LYRICS_JSON="$LYRICS_JSON" \
  python "$SCRIPT_DIR/asr.py"

  # === 6) MusicXML ===
  echo "[xml] building MusicXML -> $SCORE_XML"
  python "$SCRIPT_DIR/align_musicxml.py" \
    --midi_dir "$MIDI_DIR" \
    --lyrics_json "$LYRICS_JSON" \
    --output "$SCORE_XML" \
    --tempo 120

  # === 7) PDF (MuseScore CLI; MSCORE_BIN は環境側で設定) ===
  if [ -n "${MSCORE_BIN:-}" ]; then
    echo "[pdf] $MSCORE_BIN $SCORE_XML -> $SCORE_PDF"
    "$MSCORE_BIN" "$SCORE_XML" -o "$SCORE_PDF" >/dev/null 2>&1 || \
      echo "[warn] PDF export failed."
  else
    echo "[info] MSCORE_BIN not set; skipped PDF export."
  fi

  echo "DONE:"
  echo " - Stems    : $STEM_DIR"
  echo " - MusicXML : $SCORE_XML"
  [ -n "${MSCORE_BIN:-}" ] && echo " - PDF      : $SCORE_PDF"
#+end_src

** ensemble_stems.py（中央値 / trimmed mean アンサンブル）

#+begin_src python :tangle env/common/scripts/ensemble_stems.py :shebang "#!/usr/bin/env python3"
  import argparse
  from pathlib import Path
  from typing import List

  import numpy as np
  import soundfile as sf


  STEMS = ["vocals", "bass", "drums", "other", "guitar", "piano"]


  def load_stereo(path: Path):
      """Load audio as float32 stereo."""
      if not path.exists():
          return None, None
      y, sr = sf.read(path)
      if y.ndim == 1:
          y = np.stack([y, y], axis=0)  # (2, n)
      elif y.ndim == 2:
          # soundfile: (n, ch) → (ch, n)
          if y.shape[1] <= 4:  # assume (n, ch)
              y = y.T
      else:
          raise ValueError(f"Unexpected shape {y.shape} for {path}")
      return y.astype(np.float32), sr


  def pad_to_max(waves: List[np.ndarray]) -> np.ndarray:
      """Pad all waveforms to the max length."""
      maxlen = max(w.shape[1] for w in waves)
      padded = []
      for w in waves:
          if w.shape[1] < maxlen:
              z = np.zeros((2, maxlen), dtype=w.dtype)
              z[:, : w.shape[1]] = w
              w = z
          padded.append(w)
      return np.stack(padded, axis=0)  # (k, 2, n)


  def median_fuse(waves: List[np.ndarray]) -> np.ndarray:
      stack = pad_to_max(waves)
      return np.median(stack, axis=0)  # (2, n)


  def tmean_fuse(waves: List[np.ndarray], alpha: float) -> np.ndarray:
      stack = pad_to_max(waves)
      k = stack.shape[0]
      lo = int(alpha * k)
      hi = k - lo
      stack_sorted = np.sort(stack, axis=0)
      trimmed = stack_sorted[lo:hi]
      return trimmed.mean(axis=0)


  def main():
      ap = argparse.ArgumentParser()
      ap.add_argument("--models", required=True, help="space-separated model names")
      ap.add_argument(
          "--stem-root", required=True, help="root where model/track/stem.wav lives"
      )
      ap.add_argument(
          "--track", required=True, help="basename of track (without extension)"
      )
      ap.add_argument("--out-dir", required=True, help="output dir for ensembled stems")
      ap.add_argument("--method", default="median", choices=["median", "tmean"])
      ap.add_argument("--tmean-alpha", type=float, default=0.1)
      args = ap.parse_args()

      models = [m for m in args.models.split() if m]
      stem_root = Path(args.stem_root)
      out_track_dir = Path(args.out_dir) / args.track
      out_track_dir.mkdir(parents=True, exist_ok=True)

      print(f"[ensemble] models={models}")
      print(f"[ensemble] stem_root={stem_root}")
      print(f"[ensemble] out_dir={out_track_dir}")

      for stem in STEMS:
          cand: List[np.ndarray] = []
          sr_ref = None

          print(f"[ensemble] === Processing stem: {stem} ===")

          for m in models:
              # Pattern A: <root>/<model>/<track>/<stem>.wav
              p1 = stem_root / m / args.track / f"{stem}.wav"
              # Pattern B: <root>/<model>/<model>/<track>/<stem>.wav (Demucs 5+)
              p2 = stem_root / m / m / args.track / f"{stem}.wav"

              for p in (p1, p2):
                  y, sr = load_stereo(p)
                  if y is not None:
                      print(f"[ensemble] found: {p}")
                      if sr_ref is None:
                          sr_ref = sr
                      cand.append(y)
                      break
              else:
                  print(f"[ensemble] NOT found for model {m}: p1={p1}, p2={p2}")

          if not cand:
              print(f"[ensemble] --- No candidates for {stem}, skipping.")
              continue

          # Fuse
          if args.method == "tmean":
              y_out = tmean_fuse(cand, args.tmean_alpha)
          else:
              y_out = median_fuse(cand)

          out_path = out_track_dir / f"{stem}.wav"
          sf.write(out_path.as_posix(), y_out.T, sr_ref, subtype="PCM_16")
          print(f"[ensemble] wrote: {out_path}  (from {len(cand)} model(s))")


  if __name__ == "__main__":
      main()
#+end_src

** asr.py（Whisper large-v3 単語タイムスタンプ）

#+begin_src python :tangle env/common/scripts/asr.py :shebang "#!/usr/bin/env python3"
  import json
  import os
  from pathlib import Path
  from faster_whisper import WhisperModel

  def main() -> None:
      audio = os.environ.get("ASR_SRC")
      out_json = os.environ.get("LYRICS_JSON")
      if not audio or not out_json:
          raise SystemExit("ASR_SRC / LYRICS_JSON must be set in environment.")

      model = WhisperModel("large-v3", device="cpu", compute_type="int8")
      segments, info = model.transcribe(audio, word_timestamps=True, language="ja")

      words = []
      for seg in segments:
          for w in (seg.words or []):
              words.append({
                  "start": float(w.start),
                  "end": float(w.end),
                  "text": w.word,
              })

      out_path = Path(out_json)
      out_path.parent.mkdir(parents=True, exist_ok=True)
      with out_path.open("w", encoding="utf-8") as f:
          json.dump({"lang": info.language, "words": words}, f,
                    ensure_ascii=False, indent=2)

      print(f"[asr] words={len(words)} -> {out_path}")

  if __name__ == "__main__":
      main()
#+end_src

** align_musicxml.py（MIDI × 歌詞を MusicXML へ）

#+begin_src python :tangle env/common/scripts/align_musicxml.py :shebang "#!/usr/bin/env python3"
  import argparse
  import json
  from pathlib import Path
  from typing import List, Tuple

  from music21 import converter, stream, tempo, meter, note, instrument

  STEM_ORDER = ["vocals", "bass", "drums", "other", "piano", "guitar"]

  def guess_stem_name(midipath: Path) -> str:
      name = midipath.stem.lower()
      for k in STEM_ORDER:
          if k in name:
              return k
      return name

  def load_words(json_path: Path) -> List[dict]:
      if not json_path.exists():
          return []
      data = json.loads(json_path.read_text("utf-8"))
      return data.get("words", [])

  def build_seconds_map(part: stream.Part, qpm: float) -> List[Tuple[note.Note, float, float]]:
      sec_per_quarter = 60.0 / qpm
      timeline: List[Tuple[note.Note, float, float]] = []
      cur_q = 0.0
      for el in part.flat.notesAndRests:
          dur_q = float(el.quarterLength)
          start_s = cur_q * sec_per_quarter
          end_s = (cur_q + dur_q) * sec_per_quarter
          timeline.append((el, start_s, end_s))
          cur_q += dur_q
      return timeline

  def attach_lyrics_to_part(part: stream.Part, words: List[dict], qpm: float) -> None:
      if not words:
          return
      tmap = build_seconds_map(part, qpm)
      wi = 0
      for el, s, e in tmap:
          if not isinstance(el, note.Note):
              continue
          mid = 0.5 * (s + e)
          best_idx, best_dist = None, 1e9
          for j in range(wi, min(wi + 10, len(words))):
              w = words[j]
              cs = 0.5 * (float(w["start"]) + float(w["end"]))
              d = abs(cs - mid)
              if d < best_dist:
                  best_dist = d
                  best_idx = j
          if best_idx is not None:
              el.addLyric(words[best_idx]["text"])
              wi = best_idx + 1

  def parse_midi_first_part(midi_path: Path) -> stream.Part:
      s = converter.parse(str(midi_path))
      if getattr(s, "parts", None) and len(s.parts) > 0:
          return s.parts[0]
      return s

  def main() -> None:
      ap = argparse.ArgumentParser()
      ap.add_argument("--midi_dir", required=True)
      ap.add_argument("--lyrics_json", required=True)
      ap.add_argument("--output", required=True)
      ap.add_argument("--tempo", type=float, default=120.0)
      args = ap.parse_args()

      midi_dir = Path(args.midi_dir)
      mids = sorted(midi_dir.glob("*_basic_pitch.mid"))
      if not mids:
          raise SystemExit(f"No MIDI files found in {midi_dir}")

      words = load_words(Path(args.lyrics_json))

      sc = stream.Score()
      sc.insert(0, tempo.MetronomeMark(number=args.tempo))
      sc.insert(0, meter.TimeSignature("4/4"))

      parts_added = 0
      for mpath in mids:
          stem = guess_stem_name(mpath)
          part_stream = parse_midi_first_part(mpath)
          try:
              inst = instrument.fromString(stem.capitalize())
          except Exception:
              inst = instrument.Instrument()
              inst.instrumentName = stem.capitalize()
          part_stream.insert(0, inst)
          if stem == "vocals":
              attach_lyrics_to_part(part_stream, words, qpm=args.tempo)
          sc.append(part_stream)
          parts_added += 1

      if parts_added == 0:
          raise SystemExit("No parsable MIDI parts.")

      out_path = Path(args.output)
      out_path.parent.mkdir(parents=True, exist_ok=True)
      sc.write("musicxml", fp=str(out_path))
      print(f"[xml] wrote MusicXML: {out_path}")

  if __name__ == "__main__":
      main()
#+end_src

* macOS 用環境（env/macOS）

** Makefile（macOS：brew + MuseScore.app）

#+begin_src makefile :tangle env/macOS/Makefile
  VENV := .venv
  PY   := $(VENV)/bin/python
  PIP  := $(VENV)/bin/pip

  MSCORE_BIN := /Applications/MuseScore\ 4.app/Contents/MacOS/mscore
  export MSCORE_BIN

  all: install

  # venv がなければ作る
  $(VENV)/bin/activate:
  	python3 -m venv $(VENV)

  install: $(VENV)/bin/activate
  	$(PY) -m pip install -U pip
  	$(PIP) install -r ../common/requirements.txt

  run: $(VENV)/bin/activate
  	# venv の bin を PATH に足してからシェルスクリプトを bash で実行
  	PATH="$(VENV)/bin:$$PATH" \
  	MODELS="htdemucs htdemucs_6s" \
  	ENSEMBLE_METHOD=tmean \
  	TMEAN_ALPHA=0.1 \
  	bash ../common/scripts/process.sh "../input/foo.wav"
#+end_src

* WSL2 用環境（env/wsl2）

** Makefile（WSL2：apt + MuseScore）

#+begin_src makefile :tangle env/wsl2/Makefile
  VENV := .venv
  PY   := $(VENV)/bin/python
  PIP  := $(VENV)/bin/pip

  MSCORE_BIN := /usr/bin/mscore
  export MSCORE_BIN

  all: install

  install:
  	sudo apt update
  	sudo apt install -y ffmpeg libsndfile1 musescore
  	python3 -m venv $(VENV)
  	source $(VENV)/bin/activate
  	$(PIP) install -U pip
  	$(PIP) install -r ../common/requirements.txt

  run:
  	source $(VENV)/bin/activate
  	MODELS="htdemucs htdemucs_6s" \
  	ENSEMBLE_METHOD=median \
  	../common/scripts/process.sh "../input/foo.wav"
#+end_src

* Docker 用環境（env/docker）

** Dockerfile

#+begin_src dockerfile :tangle env/docker/Dockerfile
FROM python:3.10-slim

RUN apt-get update && apt-get install -y ffmpeg libsndfile1 libgl1 && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

WORKDIR /opt/music2score

# 共通依存のみコピー
COPY env/common/requirements.txt ./requirements.txt
RUN pip install -U pip && pip install -r requirements.txt

# 共通スクリプトのみコピー（ソースはイメージ内に固定）
COPY env/common/scripts ./env/common/scripts
RUN chmod +x env/common/scripts/process.sh

# 入力音源はコンテナ外から /input にマウントする前提
ENTRYPOINT ["/opt/music2score/env/common/scripts/process.sh"]
#+end_src

** Docker 用 Makefile

#+begin_src makefile :tangle env/docker/Makefile
  IMAGE := music2score
  AUDIO ?= foo.wav

  build:
  	docker build -t $(IMAGE) -f Dockerfile ../..

  run:
  	docker run --rm \
  	  -v ../../input:/input \
  	  -v ../../out:/out \
  	  $(IMAGE) /input/$(AUDIO)
#+end_src

** docker-compose.yml

#+begin_src yaml :tangle env/docker/docker-compose.yml
  services:
    music2score:
      build:
        context: ../..
        dockerfile: env/docker/Dockerfile
      container_name: music2score
      volumes:
        - ../../input:/input
        - ../../out:/out
      command: ["/opt/music2score/env/common/scripts/process.sh", "/input/foo.wav"]
#+end_src

* 使い方メモ

1. Org を保存後、=org-babel-tangle= でファイル生成
2. macOS:
   - =cd env/macOS=
   - =make install=
   - =make run=（=input/foo.wav= を読み込んで =out/= 以下に出力）
3. WSL2:
   - =cd env/wsl2=
   - =make install=
   - =make run=
4. Docker:
   - =cd env/docker=
   - =make build=
   - =make run=
5. docker compose:
   - =cd env/docker=
   - =docker-compose --build= （=input/foo.wav= → out 以下に生成）
