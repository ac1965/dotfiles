#+TITLE: Audio → Stems → MIDI(+Lyrics) → Score Auto Pipeline
#+SUBTITLE: macOS / Linux 共通 Docker Full-Container Edition（完全版）
#+AUTHOR: YAMASHITA, Takao
#+OPTIONS: toc:3 num:nil creator:nil
#+PROPERTY: header-args :noweb no-export

* はじめに
この README は、*Docker コンテナの中だけで* 次の一連の処理を完結させるための完全版ドキュメントです。

1. *Demucs* による高精度ステム分離（複数モデル対応）
2. 複数モデル出力の *アンサンブル（median / trim-mean / quantile / best-pick）*
3. *Basic Pitch* による各パート別の MIDI 生成
4. *faster-whisper* による歌詞の word-level timestamp 抽出
5. *music21* による *MIDI × 歌詞* の MusicXML 合成
6. コンテナ内に同梱した *MuseScore AppImage* による PDF スコア生成

ホスト側（macOS / Linux）の前提は *Docker さえあればよい* という構成です。  
Python, ffmpeg, MuseScore などはすべてコンテナ内に閉じ込めます。

---

* 全体アーキテクチャ

#+begin_src mermaid
flowchart TD

A[Host Audio File<br>input/song.wav] -->|bind mount| B[Docker Container]

B --> C[Demucs (1〜N models)]
C --> D[Model-wise Stems<br>out/stems/<model>/<track>]

D --> E[Ensemble Fusion<br>median / tmean / q / best-pick]
E --> F[Final Stems<br>out/stems_ens/<track>]

F --> G[Basic Pitch<br>MIDI per stem]
G --> H[MIDI Files<br>out/midi]

F --> I[faster-whisper<br>word timestamps]
I --> J[Lyrics JSON<br>out/lyrics/lyrics_words.json]

H --> K[music21<br>MIDI×Lyrics alignment]
K --> L[MusicXML<br>out/score/score.musicxml]

L --> M[MuseScore (AppImage)<br>in Docker]
M --> N[PDF score<br>out/score/score.pdf]
#+end_src

---

* 前提環境

** ホスト OS
- macOS 13+（Apple Silicon / Intel どちらでも可）
- または Linux（Ubuntu 22.04+ など）

** 必須ソフトウェア（ホスト側）
- Docker Desktop（macOS）または Docker（Linux）

** ホスト側で *不要* なもの
- Python
- ffmpeg / sox / libsndfile
- MuseScore
- その他音楽系ライブラリ

すべてコンテナ側に閉じ込める設計です。

---

* ディレクトリ構成

プロジェクトルートの構成：

#+begin_src text
project-root/
├── input/
│   └── song.wav             # 処理したい音源ファイル
├── out/                     # 生成物 (stems, midi, lyrics, score...)
├── scripts/
│   ├── process.sh
│   ├── process-core.sh
│   ├── ensemble_stems.py
│   └── align_musicxml.py
├── Dockerfile
├── docker-compose.yaml
├── requirements.txt
└── README.org               # 本ファイル
#+end_src

*のちほど、すべてのファイル内容をこの README 内にフル出力します。*

---

* セットアップと実行

** 1) Docker イメージのビルド

#+begin_src sh
# プロジェクトルートで
docker-compose build
#+end_src

初回は Mus eScore AppImage のダウンロードと Python 依存のインストールがあるため、数分かかります。

** 2) 入力ファイルの配置

#+begin_src sh
mkdir -p input out
cp /path/to/your/song.wav input/
#+end_src

** 3) パイプラインの実行

#+begin_src sh
docker-compose run --rm music2score input/song.wav
#+end_src

実行後、ホスト側の =out/= 配下に生成物が出力されます。

#+begin_src text
out/
├── stems/
├── stems_ens/
├── midi/
├── lyrics/
└── score/
    ├── score.musicxml
    └── score.pdf
#+end_src

---

* アンサンブルの使い方

Demucs モデルを複数指定してアンサンブルすることで、SNR やブリード抑制を狙います。  
環境変数で設定し、=docker-compose run= の前に指定します。

** 例1: htdemucs + htdemucs_ft を median でアンサンブル

#+begin_src sh
MODELS="htdemucs htdemucs_ft" \
ENSEMBLE_METHOD=median \
docker-compose run --rm music2score input/song.wav
#+end_src

** 例2: htdemucs + htdemucs_6s を trim-mean（tmean）でアンサンブル

#+begin_src sh
MODELS="htdemucs htdemucs_6s" \
ENSEMBLE_METHOD=tmean \
TMEAN_ALPHA=0.1 \
docker-compose run --rm music2score input/song.wav
#+end_src

** 例3: quantile + vocals の best-pick を組み合わせる

#+begin_src sh
MODELS="htdemucs htdemucs_ft" \
ENSEMBLE_METHOD=q \
Q=0.3 \
PICK_BEST=vocals \
docker-compose run --rm music2score input/song.wav
#+end_src

- =ENSEMBLE_METHOD= :: median / mean / tmean / q をサポート
- =TMEAN_ALPHA= :: tmean 時のトリム率（0.0〜0.5）
- =Q= :: quantile の分位点（0.0〜1.0）
- =PICK_BEST= :: "vocals,bass,..." など best-pick したい stem 名のカンマ区切り

---

* Python 依存関係（コンテナ内で使用）

コンテナ内で使用する Python パッケージは =requirements.txt= として定義します。

#+begin_src conf :tangle requirements.txt
demucs>=4.0.0

basic-pitch==0.4.0
librosa==0.10.2
soundfile==0.12.1
numpy==1.26.4
pydub==0.25.1

faster-whisper==1.0.3
whisperx==3.1.1
srt==3.5.3

music21==9.1.0

scikit-learn<=1.5.1
#+end_src

Ubuntu 22.04 ベースで =python3= は 3.10 系が入るため、そのまま venv で利用します。

---

* Dockerfile（Full Container 版・完全版）

Demucs, Basic Pitch, faster-whisper, music21 に加え、MuseScore AppImage を同梱して  
コンテナ内で PDF 生成まで完結させます。

#+begin_src dockerfile :tangle Dockerfile
FROM ubuntu:22.04

ENV DEBIAN_FRONTEND=noninteractive

# --- System dependencies -------------------------------------------------
RUN apt-get update && apt-get install -y \
    ffmpeg sox libsndfile1 \
    python3 python3-venv python3-pip \
    unzip wget curl git \
    libfuse2 \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# --- Python virtualenv ---------------------------------------------------
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

COPY requirements.txt /tmp/requirements.txt
RUN pip install -U pip && pip install -r /tmp/requirements.txt

# --- MuseScore AppImage (headless PDF export) ---------------------------
# バージョンは必要に応じて更新可能。
RUN wget -O /tmp/MuseScore.AppImage \
    https://github.com/musescore/MuseScore/releases/download/v4.3.2/MuseScore-4.3.2.241231751-x86_64.AppImage

RUN chmod +x /tmp/MuseScore.AppImage

# AppImage 展開
RUN mkdir -p /opt/mscore \
    && /tmp/MuseScore.AppImage --appimage-extract -q \
    && cp -r squashfs-root/* /opt/mscore/ \
    && rm -rf squashfs-root

ENV MSCORE_BIN="/opt/mscore/AppRun"

# --- Application workdir -------------------------------------------------
WORKDIR /workspace

# スクリプト群をコンテナにコピー
COPY scripts/ ./scripts/

# 実行権限付与
RUN chmod +x scripts/process.sh scripts/process-core.sh

CMD ["bash", "scripts/process.sh"]
#+end_src

ポイント：

- ベースイメージ：=ubuntu:22.04=
- =libfuse2= を入れて AppImage が動くようにしている
- =/opt/venv= に仮想環境を作成して Python パッケージを隔離
- MuseScore AppImage を =/opt/mscore= に展開し、 =MSCORE_BIN= に登録

---

* docker-compose.yaml（完全版）

ホスト側の =input/= と =out/= をコンテナにマウントし、  
=music2score= サービスで =scripts/process.sh= をエントリポイントとします。

#+begin_src yaml :tangle docker-compose.yaml
version: "3.9"

services:
  music2score:
    build: .
    image: music2score:latest
    container_name: music2score
    working_dir: /workspace
    volumes:
      - ./input:/workspace/input
      - ./out:/workspace/out
      - ./scripts:/workspace/scripts
    entrypoint: ["bash", "scripts/process.sh"]
    # "docker-compose run music2score input/song.wav" の "input/song.wav" が
    # process.sh の第1引数として渡される。
#+end_src

---

* process.sh（エントリポイント・完全版）

コンテナ起動時に実行されるシンプルなラッパです。  
実際の処理は =process-core.sh= に委譲します。

#+begin_src sh :tangle scripts/process.sh :shebang "#!/usr/bin/env bash"
#!/usr/bin/env bash
set -euo pipefail

AUDIO_IN="${1:-}"

if [ -z "${AUDIO_IN}" ]; then
  echo "Usage: process.sh <audio file path under /workspace>"
  echo " e.g. process.sh input/song.wav"
  exit 1
fi

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
"$SCRIPT_DIR/process-core.sh" "$AUDIO_IN"
#+end_src

---

* process-core.sh（本体処理・完全版）

Demucs → アンサンブル → Basic Pitch → Whisper → MusicXML → MuseScore の  
全処理を行うスクリプトです。

#+begin_src sh :tangle scripts/process-core.sh :shebang "#!/usr/bin/env bash"
#!/usr/bin/env bash
set -euo pipefail

AUDIO_IN="${1:-}"
: "${AUDIO_IN:?Usage: process-core.sh <audio file>}"

WORK_DIR="/workspace"
OUT_DIR="$WORK_DIR/out"
TMP_DIR="$OUT_DIR/tmp_audio"
STEM_ROOT="$OUT_DIR/stems"
ENS_DIR="$OUT_DIR/stems_ens"
MIDI_DIR="$OUT_DIR/midi"
LYRICS_DIR="$OUT_DIR/lyrics"
SCORE_DIR="$OUT_DIR/score"
SCORE_XML="$SCORE_DIR/score.musicxml"
SCORE_PDF="$SCORE_DIR/score.pdf"

MODELS="${MODELS:-htdemucs}"
MSCORE_BIN="${MSCORE_BIN:-/opt/mscore/AppRun}"

mkdir -p "$OUT_DIR" "$TMP_DIR" "$STEM_ROOT" "$ENS_DIR" "$MIDI_DIR" "$LYRICS_DIR" "$SCORE_DIR"

# --- 0) フォーマット正規化 ---------------------------------------------
BASE="$(basename "$AUDIO_IN")"
BASE_NOEXT="${BASE%.*}"
WAV_IN="$TMP_DIR/${BASE_NOEXT}.wav"

echo "[normalize] $AUDIO_IN -> $WAV_IN"
ffmpeg -y -i "$AUDIO_IN" -ac 2 -ar 44100 -sample_fmt s16 "$WAV_IN" >/dev/null 2>&1

# --- 1) Demucs モデルリスト解析 ----------------------------------------
MODELS_SANE="$(printf "%s" "$MODELS" | tr '、,' ' ' | tr -s '[:space:]' ' ')"
read -r -a MODEL_ARR <<< "$MODELS_SANE"

if [ "${#MODEL_ARR[@]}" -eq 0 ]; then
  MODEL_ARR=(htdemucs)
fi

echo "[models] ${#MODEL_ARR[@]} -> ${MODEL_ARR[*]}"

# --- 2) Demucs 実行（overwrite 有効） -----------------------------------
for m in "${MODEL_ARR[@]}"; do
  echo "[demucs] model=$m"
  demucs -n "$m" --overwrite -o "$STEM_ROOT" "$WAV_IN" >/dev/null 2>&1
done

# 出力: $STEM_ROOT/<model>/<BASE_NOEXT>/*.wav

# --- 3) アンサンブル ---------------------------------------------------
if [ "${#MODEL_ARR[@]}" -ge 2 ]; then
  echo "[ensemble] models=${MODEL_ARR[*]} method=${ENSEMBLE_METHOD:-median}"
  python "/workspace/scripts/ensemble_stems.py" \
    --models "${MODELS_SANE}" \
    --stem-root "$STEM_ROOT" \
    --track "$BASE_NOEXT" \
    --out-dir "$ENS_DIR"
  STEM_DIR="$ENS_DIR/$BASE_NOEXT"
else
  STEM_DIR="$STEM_ROOT/${MODEL_ARR[0]}/$BASE_NOEXT"
fi

echo "[stems] using stems from: $STEM_DIR"

# --- 4) Basic Pitch ------------------------------------------------------
mkdir -p "$MIDI_DIR"

conv() {
  local f="$1"
  if [ -f "$f" ]; then
    echo "[basic-pitch] $f"
    basic-pitch "$MIDI_DIR" "$f" >/dev/null 2>&1 || true
  fi
}

for stem in vocals bass drums other guitar piano; do
  conv "$STEM_DIR/${stem}.wav"
done

# --- 5) faster-whisper による歌詞抽出 -----------------------------------
LYRICS_JSON="$LYRICS_DIR/lyrics_words.json"
ASR_SRC="$STEM_DIR/vocals.wav"
[ ! -f "$ASR_SRC" ] && ASR_SRC="$WAV_IN"

echo "[asr] source=$ASR_SRC"

PYTHONIOENCODING=utf-8 ASR_SRC="$ASR_SRC" LYRICS_JSON="$LYRICS_JSON" python <<'PY'
import os, json
from faster_whisper import WhisperModel

audio = os.environ["ASR_SRC"]
out_json = os.environ["LYRICS_JSON"]

model = WhisperModel("large-v3", device="cpu", compute_type="int8")
segments, info = model.transcribe(audio, word_timestamps=True, language="ja")

words = []
for seg in segments:
    for w in (seg.words or []):
        words.append({"start": float(w.start), "end": float(w.end), "text": w.word})

os.makedirs(os.path.dirname(out_json), exist_ok=True)
with open(out_json, "w", encoding="utf-8") as f:
    json.dump({"lang": info.language, "words": words}, f, ensure_ascii=False, indent=2)

print(f"[asr] {len(words)} words -> {out_json}")
PY

# --- 6) MusicXML 生成（tempo 自動検出） --------------------------------
python "/workspace/scripts/align_musicxml.py" \
  --midi_dir "$MIDI_DIR" \
  --lyrics_json "$LYRICS_JSON" \
  --output "$SCORE_XML" \
  --tempo 0

# --- 7) MuseScore による PDF 生成 --------------------------------------
if [ -x "$MSCORE_BIN" ]; then
  echo "[mscore] $MSCORE_BIN $SCORE_XML -> $SCORE_PDF"
  "$MSCORE_BIN" "$SCORE_XML" -o "$SCORE_PDF" >/dev/null 2>&1 || \
    echo "[warn] PDF generation failed."
else
  echo "[warn] MSCORE_BIN not executable; skip PDF."
fi

echo "DONE:"
echo " - MusicXML : $SCORE_XML"
[ -f "$SCORE_PDF" ] && echo " - PDF      : $SCORE_PDF"
#+end_src

---

* ensemble_stems.py（アンサンブルロジック・完全版）

- median :: 全モデル出力の中央値
- mean :: 単純平均
- tmean :: 上下をトリムした平均
- q :: 分位点ベースのアンサンブル
- pick-best :: 特定 stem だけ SNR っぽさの指標でモデル選択

#+begin_src python :tangle scripts/ensemble_stems.py :shebang "#!/usr/bin/env python3"
#!/usr/bin/env python3
import argparse
import os
from pathlib import Path

import numpy as np
import librosa
import soundfile as sf

STEMS = ["vocals", "bass", "drums", "other", "guitar", "piano"]

def load_audio(path: Path, sr: int = 44100):
    if not path.exists():
        return None
    y, _ = librosa.load(path.as_posix(), sr=sr, mono=False)
    if y.ndim == 1:
        y = np.stack([y, y], axis=0)
    return y

def pad_to_max(waves):
    maxlen = max(w.shape[1] for w in waves)
    padded = []
    for w in waves:
        if w.shape[1] < maxlen:
            z = np.zeros((w.shape[0], maxlen), dtype=w.dtype)
            z[:, :w.shape[1]] = w
            w = z
        padded.append(w)
    return np.stack(padded, axis=0)  # (K, C, T)

def fuse_stack(stack, method: str, alpha: float, q: float):
    if method == "median":
        return np.median(stack, axis=0)
    if method == "mean":
        return np.mean(stack, axis=0)
    if method == "tmean":
        k = stack.shape[0]
        lo = int(alpha * k)
        hi = int((1.0 - alpha) * k)
        sorted_stack = np.sort(stack, axis=0)
        return np.mean(sorted_stack[lo:hi], axis=0)
    if method == "q":
        return np.quantile(stack, q, axis=0)
    raise ValueError(f"Unknown method: {method}")

def snr_like(wave):
    # 簡易的に RMS を指標とし、より「はっきり」しているものを好む
    return float(np.sqrt(np.mean(wave**2, axis=1)).mean())

def pick_best(waves):
    scores = [snr_like(w) for w in waves]
    idx = int(np.argmax(scores))
    return waves[idx]

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--models", required=True, help="space-separated model names")
    ap.add_argument("--stem-root", required=True, help="Demucs root dir")
    ap.add_argument("--track", required=True, help="basename of track (no ext)")
    ap.add_argument("--out-dir", required=True, help="output directory for ensembled stems")
    ap.add_argument("--sr", type=int, default=44100)
    ap.add_argument("--method", default=None, help="median/mean/tmean/q (default from ENSEMBLE_METHOD)")
    ap.add_argument("--tmean-alpha", type=float, default=None, help="alpha for tmean (0~0.5)")
    ap.add_argument("--q", type=float, default=None, help="quantile for q (0~1)")
    ap.add_argument("--pick-best", default=None, help="comma-separated stems to best-pick (e.g. vocals,bass)")
    args = ap.parse_args()

    models = [m for m in args.models.split() if m.strip()]
    if not models:
        raise SystemExit("No models provided.")

    method = args.method or os.environ.get("ENSEMBLE_METHOD", "median")
    alpha = args.tmean_alpha if args.tmean_alpha is not None else float(os.environ.get("TMEAN_ALPHA", 0.1))
    q = args.q if args.q is not None else float(os.environ.get("Q", 0.5))
    pick_best_list = []
    if args.pick_best:
        pick_best_list = [s.strip() for s in args.pick_best.split(",") if s.strip()]
    else:
        env_pb = os.environ.get("PICK_BEST", "")
        if env_pb:
            pick_best_list = [s.strip() for s in env_pb.split(",") if s.strip()]
    pick_best_set = set(pick_best_list)

    stem_root = Path(args.stem_root)
    out_track_dir = Path(args.out_dir) / args.track
    out_track_dir.mkdir(parents=True, exist_ok=True)

    sr = args.sr

    print(f"[ensemble] models={models} method={method} alpha={alpha} q={q} pick_best={pick_best_set}")

    for stem in STEMS:
        waves = []
        for m in models:
            p = stem_root / m / args.track / f"{stem}.wav"
            y = load_audio(p, sr=sr)
            if y is not None:
                waves.append(y)
        if not waves:
            continue

        if len(waves) == 1:
            fused = waves[0]
            print(f"[ensemble] {stem}: single model -> passthrough")
        elif stem in pick_best_set:
            fused = pick_best(waves)
            print(f"[ensemble] {stem}: best-pick from {len(waves)} models")
        else:
            stack = pad_to_max(waves)
            fused = fuse_stack(stack, method=method, alpha=alpha, q=q)
            print(f"[ensemble] {stem}: fused {len(waves)} models with {method}")

        out_path = out_track_dir / f"{stem}.wav"
        sf.write(out_path.as_posix(), fused.T, sr, subtype="PCM_16")

if __name__ == "__main__":
    main()
#+end_src

---

* align_musicxml.py（MIDI×歌詞→MusicXML / tempo 自動検出・完全版）

- MIDI 内の MetronomeMark からテンポを検出（なければ 120 BPM）
- 歌詞は “音符の中心時間” とワード区間の最近距離で割当て
- =--tempo= で明示指定すれば固定テンポにもできる

#+begin_src python :tangle scripts/align_musicxml.py :shebang "#!/usr/bin/env python3"
#!/usr/bin/env python3
import argparse
import json
from pathlib import Path

from music21 import converter, stream, tempo, instrument, note, meter

STEMS_ORDER = ["vocals", "bass", "drums", "other", "piano", "guitar"]

def guess_stem_name(midipath: Path) -> str:
    name = midipath.stem.lower()
    for k in STEMS_ORDER:
        if k in name:
            return k
    return name

def load_words(json_path: Path):
    if not json_path.exists():
        return []
    data = json.loads(json_path.read_text("utf-8"))
    return data.get("words", [])

def detect_tempo_from_midi(midi_path: Path, fallback: float = 120.0) -> float:
    """
    MIDI 内の MetronomeMark から QPM を検出。
    なければ fallback を返す。
    """
    try:
        s = converter.parse(str(midi_path))
    except Exception:
        return fallback

    mms = s.recurse().getElementsByClass(tempo.MetronomeMark)
    for mm in mms:
        if mm.number is not None:
            try:
                return float(mm.number)
            except Exception:
                pass
        try:
            q = mm.getQuarterBPM()
            if q is not None:
                return float(q)
        except Exception:
            continue
    return fallback

def build_seconds_map(m21_part, qpm: float):
    sec_per_quarter = 60.0 / qpm
    timeline = []
    cur_q = 0.0
    for el in m21_part.flat.notesAndRests:
        dur_q = float(el.quarterLength)
        start_s = cur_q * sec_per_quarter
        end_s = (cur_q + dur_q) * sec_per_quarter
        timeline.append((el, start_s, end_s))
        cur_q += dur_q
    return timeline

def attach_lyrics_to_part(m21_part, words, qpm: float):
    if not words:
        return
    tmap = build_seconds_map(m21_part, qpm)
    wi = 0
    for el, s, e in tmap:
        if not isinstance(el, note.Note):
            continue
        mid = 0.5 * (s + e)
        best_j, best_dist = None, 1e9
        for j in range(wi, min(wi + 8, len(words))):
            w = words[j]
            ws, we = float(w["start"]), float(w["end"])
            if ws <= mid <= we:
                best_j, best_dist = j, 0.0
                break
            center = 0.5 * (ws + we)
            dist = abs(center - mid)
            if dist < best_dist:
                best_j, best_dist = j, dist
        if best_j is not None:
            el.addLyric(words[best_j]["text"])
            wi = best_j + 1

def parse_midi_first_part(midi_path: Path):
    s = converter.parse(str(midi_path))
    if getattr(s, "parts", None) and len(s.parts) > 0:
        return s.parts[0]
    return s

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--midi_dir", required=True)
    ap.add_argument("--lyrics_json", required=True)
    ap.add_argument("--output", required=True)
    ap.add_argument("--tempo", type=float, default=0.0,
                    help="QPM. >0 なら固定テンポ、<=0 なら MIDI から自動検出（fallback 120）")
    args = ap.parse_args()

    midi_dir = Path(args.midi_dir)
    output = Path(args.output)
    output.parent.mkdir(parents=True, exist_ok=True)

    midis = sorted(midi_dir.glob("*_basic_pitch.mid"))
    if not midis:
        raise SystemExit(f"No MIDI files found in {midi_dir}")

    # tempo 決定
    if args.tempo > 0:
        qpm = float(args.tempo)
        print(f"[tempo] using user tempo: qpm={qpm}")
    else:
        qpm = detect_tempo_from_midi(midis[0], fallback=120.0)
        print(f"[tempo] auto-detected from {midis[0].name}: qpm={qpm}")

    words = load_words(Path(args.lyrics_json))

    sc = stream.Score()
    sc.insert(0, tempo.MetronomeMark(number=qpm))
    sc.insert(0, meter.TimeSignature("4/4"))

    parts_added = 0
    for mpath in midis:
        stem = guess_stem_name(mpath)
        part_stream = parse_midi_first_part(mpath)
        try:
            inst = instrument.fromString(stem.capitalize())
        except Exception:
            inst = instrument.Instrument()
            inst.instrumentName = stem.capitalize()
        part_stream.insert(0, inst)
        if stem == "vocals":
            attach_lyrics_to_part(part_stream, words, qpm=qpm)
        sc.append(part_stream)
        parts_added += 1

    if parts_added == 0:
        raise SystemExit("No parsable parts from MIDI.")

    sc.write("musicxml", fp=str(output))
    print(f"[xml] Wrote MusicXML: {output}")

if __name__ == "__main__":
    main()
#+end_src

---

* （任意）Makefile

ローカル開発時に

- venv 作成
- 依存パッケージ導入
- テスト実行

などを簡略化する Makefile 例です。Docker 版では必須ではありません。

#+begin_src makefile :tangle Makefile
VENV := .venv
PY   := $(VENV)/bin/python
PIP  := $(VENV)/bin/pip

all: venv deps

venv:
	@test -d $(VENV) || python3 -m venv $(VENV)

deps: venv requirements.txt
	$(PIP) install -U pip
	$(PIP) install -r requirements.txt

test-asr: venv
	$(PY) - <<'PY'
from faster_whisper import WhisperModel
m = WhisperModel("large-v3", device="cpu", compute_type="int8")
print("Whisper OK:", m)
PY

clean:
	rm -rf out
#+end_src

---

* トラブルシューティング

** Demucs モデルが見つからない / 遅い
- 初回実行時に自動ダウンロードされる
- CPU でも動作するが、曲が長いと時間がかかる
- モデル数を減らしたい場合は =MODELS= を 1つ（例: =htdemucs=）にする

** PDF が生成されない
- MuseScore AppImage が起動に失敗した場合、ログに =[warn] PDF generation failed.= が出る
- バージョンを変えたい場合は Dockerfile 内の AppImage URL を更新する

** 歌詞同期が甘い
- 本実装は「一定テンポ + 音符中心時間と単語区間の最小距離」の簡易同期
- 改善したい場合は：
  - WhisperX の forced alignment を導入
  - music21 側でテンポ変化を扱う実装に拡張

---

* まとめ
この README は、macOS / Linux 上で Docker を用いて

- Demucs アンサンブル
- Basic Pitch MIDI 化
- faster-whisper 歌詞タイムスタンプ
- music21 による MusicXML 合成
- MuseScore AppImage による PDF 出力

を *完全にコンテナ内で完結* させるための、  
省略なし・スクリプト全文付きの完全版です。

このファイルをそのまま保存し、

- =org-babel-tangle= により各ファイルを生成
- =docker-compose build=
- =docker-compose run --rm music2score input/song.wav=

とするだけで、同じ環境を何度でも再現できます。
